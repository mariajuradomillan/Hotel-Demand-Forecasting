---
title: "TFM"
author: "María Jurado Millán"
date: "2024-04-14"
output: html_document
---

## DEMAND FORECASTING FOR HOTEL INDUSTRY

### LIBRARIES

```{r , message=FALSE, echo=FALSE}

library(tidyverse)
library(dplyr)
library(readr)
library(plotly)
library(caret)
library(mice)
library(dplyr)
library(ggplot2)
library(gghighlight)
library(hrbrthemes)
library(patchwork)
library(leaflet)
library(MASS)
library(xgboost)
library(tidyverse)
library(GGally) 
library(factoextra) 
library(mice)
library(plotly)
library(rpart)


```

### DATA CLEANING AND DESCRIPTIVE TOOLS

The dataset contains data from a resort hotel located in Algarve and is formed by 31 variables and 40,060 observations. Each observation represents a hotel booking. It comprehend bookings due to arrive between the 1st of July of 2015 and the 31st of August 2017, including bookings that effectively arrived and bookings that were canceled. 

The data-set was obtained from ScienceDirect. The variables are:

-   **IsCanceled**: value inValue indicating if the booking was canceled (1) or not (0)

-   **ADR**: Average Daily Rate. Calculated by dividing the sum of all lodging transactions by the total number of staying nights.

-   **LeadTime:** number of days that elapsed between the entering date of the booking into the PMS and the arrival date.

-   **ArrivalDateYear**: year of arrival date.

-   **ArrivalDateMonth**: month of arrival date with 12 categories: "January" to "December"

-   **ArrivalDateWeek**: week number of the arrival date.

-   **ArrivalDateDayMonth**: day of the month of the arrival date.

-   **StaysInWeekNights**: number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel.

-   **StaysInWeekendNights**: number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel.

-   **Adults**: number of adults.

-   **Children**: number of children.

-   **Babies**: number of babies.

-   **Meal**: type of meal booked. Categories are presented in standard hospitality meal packages:

    -   Undefined/SC -- no meal package

    -   BB -- Bed & Breakfast

    -   HB -- Half board (breakfast and one other meal -- usually dinner)

    -   FB -- Full board (breakfast, lunch and dinner)

-   **Country**: country of origin. Categories are represented in the ISO 3155--3:2013 format

-   **Market segment**: market segment designation. In categories, the term "TA" means "Travel Agents" and "TO" means "Tour Operators",

-   **DistributionChannel**: booking distribution channel. The term "TA" means "Travel Agents" and "TO" means "Tour Operators"

-   **IsRepeatedGuest**: value indicating if the booking name was from a repeated guest (1) or not (0)

-   **PreviousCancellation**: number of previous bookings cancelled by the customer prior to the current booking. In case there was no customer profile associated with the booking, the value is set to 0. Otherwise, the value is the number of bookings with the same customer profile created before the current booking and not canceled.

-   **ReservedRoomType**: code of room type reserved. Code is presented instead of designation for anonymity reasons.

-   **AssignedRoomType**: code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons.

-   **BookingChanges**: number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation.

-   **DepositType**: indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories

    -   No Deposit -- no deposit was made

    calculated based on the payments identified for the booking in the transaction (TR) table before the booking׳s arrival or cancellation date.

    -   Non Refund -- a deposit was made in the value of the total stay cost

    -   Refundable -- a deposit was made with a value under the total cost of stay.

    Otherwise the value is set as "Refundable"

-   **Agent**: ID of the travel agency that made the booking.

-   **Company**: ID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons.

-   **DaysWaitingList**: number of days the booking was in the waiting list before it was confirmed to the customer.

-   **CustomerType**: type of booking, assuming one of four categories:

    -   Contract - when the booking has an allotment or other type of contract associated to it;

    -   Group -- when the booking is associated to a group

    -   Transient -- when the booking is not part of a group or contract, and is not associated to other transient booking

    -   Transient-party -- when the booking is transient, but is associated to at least other transient booking

-   **RequiredParking**: number of car parking spaces required by the customer.

-   **TotalSpecialRequests**: number of special requests made by the customer (e.g. twin bed or high floor)

-   **ReservationStatus**: reservation last status, assuming one of three categories:

    -   Canceled -- booking was canceled by the customer

    -   Check-Out -- customer has checked in but already departed

    -   No-Show -- customer did not check-in and did inform the hotel of the reason why

-   **ReservationStatusDate**: date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel.

#### Data Cleaning

1.  Load the dataset

```{r}
data<-read.csv("H1.csv")
```

2.  Any missing data?

```{r}
summary(is.na(data))

```

3.  Variables transformation

IsCanceled is converted to a categorical variable

```{r}
data<-data |> 
  mutate(IsCanceled= case_when(
    IsCanceled==0 ~ "No",
    IsCanceled==1 ~ "Yes",
    TRUE ~ "Na"
  ))
```

Some variables are not detected as categorical, so they must be transformed

```{r}
categorical_vars <- c("ArrivalDateMonth", "Meal",  "MarketSegment",
                      "DepositType", "CustomerType", "ReservationStatus")

data <- data %>%
  mutate(across(all_of(categorical_vars), as.factor))
```

4.  Variables creation

A new variable that quantifies the total number of guests is created.

```{r}
data <- data %>%
  mutate(TotalGuests = Adults + Children + Babies)
```

A binary variable is created to register if the reserved room type matches the assigned room type.

```{r}
data<- data |> 
  mutate(ExpectedRoomType= case_when(
ReservedRoomType==AssignedRoomType ~ 1,
TRUE ~ 0))

```

5.  Eliminate redundant information

For the purpose of the study, those variables that are not giving extra information or that are already combined into a new one can be eliminated to avoid multicollinearity.

```{r}
data <- subset(data, select = -c(Adults, Children, Babies, Company, ReservationStatus, ReservedRoomType, 
                                  AssignedRoomType, Agent, ReservationStatusDate, 
                                  ArrivalDateWeekNumber, ArrivalDateDayOfMonth, Country, DistributionChannel))


```

#### Descriptive Analytics

Total guests distribution

```{r}
mean_total_guests <- mean(data$TotalGuests, na.rm = TRUE)
numg <- ggplot(data, aes(x = TotalGuests)) +
  geom_histogram(binwidth = 1, fill = "cyan3", color = "black", aes(y = ..count..)) +
  geom_vline(aes(xintercept = mean_total_guests), color = "red", linetype = "dashed", size = 1) +
  geom_text(aes(x = mean_total_guests, y = Inf, label = paste("Mean =", round(mean_total_guests, 2))),
            color = "red", vjust = -0.5, hjust = 1.2, size = 4) +
  labs(title = "GRAPH 1: Distribution of Total Guests",
       x = "Total Guests",
       y = "Count",
       subtitle = "Histogram of Total Guests with Mean Indicated") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  scale_x_continuous(limits = c(0, 5), breaks = 0:5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))


numg

```

The average number of total guests per booking is 2, with the mode also being 2. This indicates that the most common reservation at this hotel typically involves two guests. The hotel accommodates bookings ranging from 1 to 4 guests, with reservations for 4 guests being the least frequent. This descriptive analysis provides valuable insights into the hotel's customer demographics, helping to identify the typical "user persona." Understanding these patterns is crucial for tailoring marketing strategies and improving demand forecasting accuracy by focusing on the predominant customer segments and reservation characteristics.

•Bookings per customer type by year

```{r}
data |> 
  group_by(ArrivalDateYear, CustomerType) |> 
  summarize(Count = n()) |> 
ggplot(aes(fill=CustomerType, y=Count, x=ArrivalDateYear)) + 
    geom_bar(position="dodge", stat="identity") +
  theme(plot.background = element_rect(fill = "white"), panel.background = element_rect(fill = "white")) +
  labs(x= "Year", y="Total number of bookings", title="GRAPH 2: Yearly bookings per customer type ")
```

These hotels increased significantly the amount of guests from 2015 to 2016. However, in 2017 the overall demand decreased compared with 2016.

Group customers are the minority type of customers that these hotels receive but their demand keeps constant thought the years. Contract customers have decreased since 2015 but they do not represent a high percentage of bookings. Something has happened in 2017 and further information will be necessary to understand what happened and why the demand has decreased. It is important to take into account that this graph shows the number of bookings received, but not all of them are successfully completed.

•Cancellations and successful bookings

```{r}
data |> 
  group_by(ArrivalDateYear, IsCanceled) |> 
  summarize(Count = n()) |> 
ggplot(aes(y = as.factor(ArrivalDateYear), x = Count, fill = IsCanceled)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_discrete(labels = c("2015", "2016", "2017")) +  # Set labels for y-axis
  scale_fill_manual(values = c("No" = "yellowgreen", "Yes" = "tomato1"), name = "Is Canceled") +  # Set colors for bars
  geom_vline(xintercept = c(0.5, 1.5), color = c("yellowgreen", "tomato1")) +     
  labs(x = "Count of Bookings", y = "Year", title="GRAPH 3: Cancellations and successful bookings over the years") + theme(plot.background = element_rect(fill = "white"), panel.background = element_rect(fill = "white"))
```

In 2015 around one third of bookings were cancelled. In 2016 as we have seen in the previous graph, is the year with more bookings received and it also has the higher number of cancellations. While in 2017 the total amount of cancellations is in the middle between 2015 and 2016.

This information is analyzed regarding the total amount of cancellations, but each year received a different number of total bookings so it would be more informative to calculate the proportion of cancellations regarding total amount of bookings

•Proportion of cancellations

```{r}
data |> 
  group_by(ArrivalDateYear) |> 
  mutate(TotalBookings = n()) |> 
  group_by(ArrivalDateYear, IsCanceled) |> 
  summarize(Count = n()) |> 
  left_join(data |> 
              group_by(ArrivalDateYear) |> 
              summarize(TotalBookings = n()), by = "ArrivalDateYear") |> 
  mutate(ProportionCancelled = if_else(IsCanceled == "Yes", Count / TotalBookings, 0))

```

In 2017 30.76% of bookings were cancelled, while in 2016 there were 26.55% of cancellations and 25.71% in 2015. The percentage of cancellations has increased over the years, but the total number of reservations has increased too.

•Nº of bookings by month

```{r}
month_order <- c("January", "February", "March", "April", "May", "June", 
                 "July", "August", "September", "October", "November", "December")

f_data1 <- data |> 
  filter(IsCanceled == "No" & ArrivalDateYear == 2015) |>
  group_by(ArrivalDateMonth) |>
  summarize(Count = n()) |>
  mutate(ArrivalDateMonth = factor(ArrivalDateMonth, levels = month_order))

f_data1$ArrivalDateMonth <- factor(f_data1$ArrivalDateMonth, levels = month_order)


plot1 <- ggplot(f_data1, aes(x = ArrivalDateMonth, y = Count, group = 1)) +
  geom_line(size = 0.4, color = '#4393C3') +
  geom_point(color = '#4393C3', size = 3) +  
  theme_bw() +
  labs(x = "Month", y = "Count", title = "2015") +
  theme(plot.title = element_text(hjust = 0.5))

f_data2 <- data |> 
  filter(IsCanceled == "No" & ArrivalDateYear == 2016) |>
  group_by(ArrivalDateMonth) |>
  summarize(Count = n()) |>
  mutate(ArrivalDateMonth = factor(ArrivalDateMonth, levels = month_order))

f_data1$ArrivalDateMonth <- factor(f_data1$ArrivalDateMonth, levels = month_order)

plot2 <- ggplot(f_data2, aes(x = ArrivalDateMonth, y = Count, group = 1)) +
  geom_line(color = '#4393C3', size = 0.4) +
  geom_point(color = '#4393C3', size = 3) +  
  theme_bw() +
  labs(x = "Month", y = "Count", title = "2016") +
  theme(plot.title = element_text(hjust = 0.5)) 

f_data3 <- data |> 
  filter(IsCanceled == "No" & ArrivalDateYear == 2017) |>
  group_by(ArrivalDateMonth) |>
  summarize(Count = n()) |>
  mutate(ArrivalDateMonth = factor(ArrivalDateMonth, levels = month_order))

f_data1$ArrivalDateMonth <- factor(f_data1$ArrivalDateMonth, levels = month_order)

plot3 <- ggplot(f_data3, aes(x = ArrivalDateMonth, y = Count, group = 1)) +
  geom_line(color = '#4393C3', size = 0.4) +
  geom_point(color = '#4393C3', size = 3) +  
  theme_bw() +
  labs(x = "Month", y = "Count", title = "2017") +
  theme(plot.title = element_text(hjust = 0.5))  

plot1 / plot2 / plot3 + plot_annotation(title = 'GRAPH 4: Monthly arrivals over the years')
```

### Scale numerical variables

```{r}
numeric_vars <- c("LeadTime", "ArrivalDateYear", "StaysInWeekendNights", "StaysInWeekNights",
                  "DaysInWaitingList", "ADR", "RequiredCarParkingSpaces", "TotalOfSpecialRequests")

data[numeric_vars] <- scale(data[numeric_vars])


```

### CLASSIFICATION

#### Benchmark

The target would be IsCanceled, and in this case success means that the booking is not cancelled.

```{r}
data$Success <- factor(data$IsCanceled == "No")
data$IsCanceled <- NULL
prop.table(table(data$Success))
```

If a new reservation is booked, the actual data will predict that 72.23% it will be a success and 27.76% a cancellation. Then, for the final model we want an accuracy higher than 72.23%.

#### Data Splitting

```{r}
set.seed(2020)
in_train <- createDataPartition(data$Success, p = 0.2, list = FALSE)  
training <- data[ in_train,]
testing <- data[-in_train,]

levels(training$Success) <- c("No", "Yes")
levels(testing$Success) <- c("No", "Yes")
```

### Logistic Regression

#### Function to evaluate logistic regression

```{r}
# Logistic Regression Evaluation Function
evaluate_logit <- function(model, testing, threshold = 0.5) {
  set.seed(123)

  # Predict probabilities
  predicted_prob <- predict(model, newdata = testing, type = "response")
  
  # Predict classes based on the threshold
  predicted_class <- ifelse(predicted_prob > threshold, "Yes", "No")
  
  # Ensure Success is a factor with levels "Yes" and "No"
  actual_class <- factor(testing$Success, levels = c("No", "Yes"))
  predicted_class <- factor(predicted_class, levels = c("No", "Yes"))
  
  # Debugging output
  print("Actual classes:")
  print(table(actual_class))
  print("Predicted classes:")
  print(table(predicted_class))
  
  # Confusion Matrix
  confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
  
  # Debugging output
  print("Confusion Matrix:")
  print(confusion_matrix)
  
  # Ensure confusion matrix has expected dimensions
  if (all(dim(confusion_matrix) == c(2, 2))) {
    accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    precision <- confusion_matrix["Yes", "Yes"] / sum(confusion_matrix["Yes", ])
    recall <- confusion_matrix["Yes", "Yes"] / sum(confusion_matrix[, "Yes"])
    f1 <- 2 * (precision * recall) / (precision + recall)
  } else {
    accuracy <- NA
    precision <- NA
    recall <- NA
    f1 <- NA
  }
  
  # Create a dataframe to hold all the metrics
  metrics <- data.frame(
    Metric = c("Accuracy", "Precision", "Recall", "F1"),
    Value = c(accuracy, precision, recall, f1)
  )
  
  # Return the metrics dataframe
  return(metrics)
}



```

**Model performance**

```{r , message=FALSE}
set.seed(123)
logit.model = glm(Success ~ ., data = training, family = binomial)

logit.evaluation= evaluate_logit(logit.model, testing)
logit.evaluation

```

#### Confusion matrix

```{r}
set.seed(123)

predicted_prob <- predict(logit.model, type = "response")

predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

actual_class <- ifelse(training$Success == "TRUE", 1, 0)

confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
confusion_matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy

```

### Bayes Classifier

```{r}
set.seed(123)
lda.model = lda(Success ~ ., data = training)
lda.model
```

```{r}
set.seed(123)
probability3 = predict(lda.model, newdata=testing)$posterior
head(probability3)
```

The first observation has a probability of 72.08% of success.

```{r}
set.seed(123)
prediction3 = predict(lda.model, newdata=testing)$class
```

```{r}
confusionMatrix(prediction3, testing$Success)$table
confusionMatrix(prediction3, testing$Success)$overall[1:2]
```

**Function to evaluate Bayes Classifier**

```{r}
# LDA Evaluation Function
evaluate_lda_model <- function(model, testing) {
  # Predict classes
  class_pred <- predict(model, newdata = testing)$class
  
  # Ensure the levels of the predicted and actual classes match
  class_pred <- factor(class_pred, levels = levels(testing$Success))
  
  # Confusion Matrix
  confusion <- confusionMatrix(class_pred, testing$Success)
  
  # Extract metrics
  accuracy <- confusion$overall['Accuracy']
  precision <- confusion$byClass['Precision']
  recall <- confusion$byClass['Sensitivity']
  f1 <- confusion$byClass['F1']
  
  # Create a dataframe to hold all the metrics
  metrics <- data.frame(
    Metric = c("Accuracy", "Precision", "Recall", "F1"),
    Value = c(accuracy, precision, recall, f1)
  )
  
  # Return the metrics dataframe
  return(metrics)
}

```

```{r}
lda_evaluation <- evaluate_lda_model(lda.model, testing)
lda_evaluation
```

For the next models, instead of data splitting, cross-validation is performed. So a control function must be defined.

### Cross-validation

Control function

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5,
                     repeats = 3,
                     classProbs = T,
                     summaryFunction=twoClassSummary,
                     verboseIter = T)

levels(training$Success)=c("No","Yes")
levels(testing$Success)=c("No","Yes")
```

```{r}
set.seed(123)
param_grid = expand.grid(gamma = seq(0, 1, 0.1), lambda = seq(0.1, 0.9, 0.1))
ldaFit <- train(Success ~ ., 
                method ="rda", 
                data = training, 
                tuneGrid = param_grid,
                preProcess = c("center", "scale"),
                metric="ROC",
                trControl = ctrl)
#print(ldaFit)
```

According to cross-validation, the optimal values are gamma=0 and lambda 0.9

Let's predict and validate

```{r}
set.seed(2020)
ldaPred = predict(ldaFit, testing)
confusionMatrix(ldaPred, testing$Success)
```

This result is not very convincing, the accuracy has decreased (67.38%), the amount of false positives have decresed (1097) and true negatives have increased (7800) but the amount of false negatives has increased a lot (9358). INT BIEN

To decrease the prediction of successful bookings as cancellations, seems reasonable to decrease the threshold to have less false negatives.

```{r}
threshold = 0.6
ldaProb = predict(ldaFit, testing, type="prob")
prediction4 <- as.factor(ifelse(ldaProb[,2] > threshold, "Yes", "No"))

confusionMatrix(prediction4, testing$Success)$table
confusionMatrix(prediction4, testing$Success)$overall[1:2]
```

Accuracy has increased compared to previous estimation but it is still lower than other models performed.

```{r}
set.seed(123)
lda.model <- lda(Success ~ ., data = training)
lda.evaluation <- evaluate_lda_model(lda.model, testing)
lda.evaluation
```

#### Function to evaluate machine learning models

```{r}
# General Machine Learning Models Evaluation Function
evaluate_ml <- function(model, testing, threshold = 0.5) {
  # Predict probabilities
  prob <- predict(model, newdata = testing, type = "prob")
  
  # Predict classes based on the threshold
  class_pred <- ifelse(prob[,2] > threshold, "Yes", "No")
  
  # Ensure the levels of the predicted and actual classes match
  class_pred <- factor(class_pred, levels = levels(testing$Success))
  
  # Confusion Matrix
  confusion <- confusionMatrix(class_pred, testing$Success)
  
  # Extract metrics
  accuracy <- confusion$overall['Accuracy']
  precision <- confusion$byClass['Pos Pred Value']
  recall <- confusion$byClass['Sensitivity']
  f1 <- 2 * (precision * recall) / (precision + recall)
  
  # Create a dataframe to hold all the metrics
  metrics <- data.frame(
    Metric = c("Accuracy", "Precision", "Recall", "F1"),
    Value = c(accuracy, precision, recall, f1)
  )
  
  # Return the metrics dataframe
  return(metrics)
}

```

### KNN

```{r}
set.seed(2020)
param_grid <- expand.grid(kmax = seq(1, 21, by = 2),  
                          distance = 2,  
                          kernel = 'optimal')
knnFit <- train(Success ~ ., 
                  data = training,
                  method = "kknn",   
                  preProc=c('scale','center'),
            tuneGrid = param_grid,
                  tuneLength = 10,
                
                  metric="ROC",
                  trControl = ctrl)
plot(knnFit)
```

According to this model, the optimal number of neighbors is 21

```{r}
knn_evaluation <- evaluate_ml(knnFit, testing)
knn_evaluation
```

```{r}
set.seed(2020)
knnProb = predict(knnFit, testing, type="prob")
prediction <- as.factor(ifelse(knnProb[,2] > 0.5, "Yes", "No"))
```

```{r}
confusionMatrix(prediction, testing$Success)$table
confusionMatrix(prediction, testing$Success)$overall[1:2]
```

### Decision Tree

```{r}
set.seed(2020)
control = rpart.control(minsplit = 30, maxdepth = 10, cp=0.01)
```

```{r}
model = Success ~.
dtFit <- rpart(model, data=training, method = "class", control = control)
```

This decision tree represents the predicted classes, the predicted probability of each class and the percentage of observations in the node.

In this particular case, there are 389 nodes and each of them has different number of observations and different probabilities that can be seen above.

```{r}
control = rpart.control(minsplit = 8, maxdepth = 12, cp=0.001)
dtFit <- rpart(model, data=training, method = "class", control = control)
```

```{r}
dtPred <- predict(dtFit, testing, type = "class")

dtProb <- predict(dtFit, testing, type = "prob")

prediction <- as.factor(ifelse(dtProb[,2] > 0.5, "Yes", "No"))

```

```{r}
confusionMatrix(prediction, testing$Success)$table
confusionMatrix(prediction, testing$Success)$overall[1:2]

```

In this case, the decision tree is really good at predicting positive cases but lacks on predicting negative cases. The amount of predictions of successful bookings is really high, but the amount of false positives is also really high.

```{r}
dt_evaluation <- evaluate_ml(dtFit, testing)
dt_evaluation
```

### Random Forest

```{r}
set.seed(2020)
rfFit <- train(Success ~ ., 
                  data = training,
                  method = "rf",   
                  preProc=c('scale','center'),
                  tuneLength = 10,
                  metric="ROC",
                  trControl = ctrl)
plot(rfFit)
```

Random forest is a technique like decision tree but repeated many times.

According to Random Forest, the optimal number of variables is 10.

**Model performance**

```{r}
rf_evaluation <- evaluate_ml(rfFit, testing)
rf_evaluation
```

**Confusion matrix**

```{r}
set.seed(2020)
rfProb = predict(rfFit, testing, type="prob")
prediction <- as.factor(ifelse(rfProb[,2] > 0.5, "Yes", "No"))

confusionMatrix(prediction, testing$Success)$table
confusionMatrix(prediction, testing$Success)$overall[1:2]
```

### Gradient Boosting

```{r , message=FALSE}
set.seed(2020)

gmbFit <- train(Success~ ., 
                  data = training,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  objective="reg:squarederror",
                  trControl = ctrl,
                  tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
                                         gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
                                         min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))

plot(gmbFit)
```

**Model performance**

```{r}
gmb_evaluation <- evaluate_ml(gmbFit, testing)
gmb_evaluation
```

**Confusion matrix**

```{r}
set.seed(2020)
gbmProb = predict(gmbFit, testing, type="prob")
prediction <- as.factor(ifelse(gbmProb[,2] > 0.5, "Yes", "No"))

confusionMatrix(prediction, testing$Success)$table
confusionMatrix(prediction, testing$Success)$overall[1:2]
```

## RESULTS SUMMARY

```{r}
# Combine the evaluations into a list for easier processing
evaluations <- list(
  LogisticRegression = logit.evaluation,
  BayesClassifier = lda_evaluation,
  KNN = knn_evaluation,
  DecisionTree = dt_evaluation,
  RandomForest = rf_evaluation,
  GradientBoosting = gmb_evaluation
)

# Initialize an empty data frame
results_df <- data.frame(Model = character(), Metric = character(), Value = numeric(), stringsAsFactors = FALSE)

# Loop through each evaluation and combine into a single data frame
for (model_name in names(evaluations)) {
  eval <- evaluations[[model_name]]
  eval <- eval %>% mutate(Model = model_name)
  results_df <- bind_rows(results_df, eval)  # Use bind_rows instead of rbind for dplyr compatibility
}

# Reorder columns to have Metric, Model, and Value
results_df <- results_df %>%
  dplyr::select(Metric, Model, Value)

# Sort the data frame by Metric first, then by Value in descending order to highlight best models
results_df <- results_df %>%
  arrange(Metric, desc(Value))

# Function to highlight best models
highlight_best_models <- function(df) {
  df %>%
    group_by(Metric) %>%
    mutate(Highlight = ifelse(Value == max(Value, na.rm = TRUE), "highlight", "normal")) %>%
    ungroup()
}

# Highlight the best performing models for each metric
results_df <- highlight_best_models(results_df)

# Create a gt table with highlighted rows for best models
results_table <- gt(results_df) %>%
  tab_style(
    style = cell_fill(color = "lightblue"),
    locations = cells_body(
      rows = results_df$Highlight == "highlight"
    )
  ) %>%
  cols_hide(columns = vars(Highlight)) %>%  # Hide the Highlight column
  tab_options(
    table.font.size = pct(70),  # Adjust font size to make the table more compact
    table.width = pct(40),      # Adjust table width for better fitting
    data_row.padding = px(2),   # Adjust row padding for compactness
    summary_row.padding = px(2)
  ) %>%
  cols_width(
    everything() ~ px(40)      # Set column width to be narrow and uniform
  ) %>%
  tab_style(
    style = cell_text(align = "left"),
    locations = cells_body()
  ) %>%
  tab_style(
    style = cell_text(align = "center"),   # Center the column titles
    locations = cells_title()
  ) %>%
  tab_style(
    style = cell_text(align = "center"),   # Center the column headers
    locations = cells_column_labels()
  )

# Display the table
results_table

```
